{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 29893,
     "status": "ok",
     "timestamp": 1733481542321,
     "user": {
      "displayName": "Marthin Surya Setiawan",
      "userId": "04398797140247513367"
     },
     "user_tz": -60
    },
    "id": "2eSvM9zX_2d3"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth\n",
    "!pip install scikit-learn\n",
    "# Also get the latest nightly Unsloth!\n",
    "#!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
    "\n",
    "\n",
    "# Unsloth achieves up to 2x faster fine-tuning speeds compared to traditional methods, with a significant reduction in memory usage (up to 70%).\n",
    "# This makes it suitable for environments with constrained computational resources, like Google Colab or low-end GPUs​\n",
    "\n",
    "# Unsloth leverages LoRA (Low-Rank Adaptation), which modifies only a small fraction (1-10%) of a model's parameters during training,\n",
    "# instead of fine-tuning the entire model. This drastically reduces the computational and memory requirements while achieving comparable performance.\n",
    "# It allows models to adapt to domain-specific tasks without retraining the entire network, enabling faster iterations and greater flexibility.\n",
    "\n",
    "# By supporting 4-bit quantization, Unsloth minimizes memory usage during training and inference.\n",
    "# Quantization reduces the precision of the weights and activations, which reduces memory demands and accelerates computation while preserving accuracy​.\n",
    "#     - the weights of the models use only 4-bits representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "test_dataset = load_dataset(\"rajpurkar/squad\") #Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, \n",
    "                                          #consisting of questions posed by crowdworkers on a set of Wikipedia articles, \n",
    "                                          # where the answer to every question is a segment of text, or span, from the corresponding reading passage\n",
    "                                         #, or the question might be unanswerable.\n",
    "\n",
    "# Load fine-tuned model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emeses/lab2_model\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"emeses/lab2_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_answer(question, context):\n",
    "    \"\"\"\n",
    "    Given a question and context, return the predicted answer using the model.\n",
    "    \"\"\"\n",
    "    # Tokenize the input question and context\n",
    "    inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "\n",
    "    # Perform inference with the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get the start and end positions of the predicted answer\n",
    "    start_position = outputs.start_logits.argmax()\n",
    "    end_position = outputs.end_logits.argmax()\n",
    "\n",
    "    # Convert the token indices to the answer string\n",
    "    answer_tokens = inputs['input_ids'][0][start_position:end_position+1]\n",
    "    predicted_answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
    "\n",
    "    return predicted_answer\n",
    "\n",
    "def evaluate_model_on_subset(test_dataset, percentage_set=100):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a subset of the test dataset based on the given percentage.\n",
    "    \n",
    "    Parameters:\n",
    "    - test_dataset: The full test dataset.\n",
    "    - percentage_set: Percentage of the dataset to use for testing (0-100).\n",
    "    \"\"\"\n",
    "    # Calculate the number of samples to select based on percentage\n",
    "    total_samples = len(test_dataset['test'])\n",
    "    num_samples_to_select = int((percentage_set / 100) * total_samples)\n",
    "\n",
    "    # Randomly sample the subset from the dataset\n",
    "    subset = random.sample(test_dataset['test'], num_samples_to_select)\n",
    "\n",
    "    true_answers = []\n",
    "    predicted_answers = []\n",
    "\n",
    "    # Loop over the subset of the test dataset\n",
    "    for entry in subset:\n",
    "        question = entry['question']\n",
    "        context = entry['context']  # Get the context (passage) associated with the question\n",
    "        true_answer = entry['answers']['text'][0]  # Get the text part of the answer from the dictionary\n",
    "\n",
    "        # Get the predicted answer for this question-context pair\n",
    "        predicted_answer = get_predicted_answer(question, context)\n",
    "\n",
    "        # Append both true and predicted answers to the respective lists\n",
    "        true_answers.append(true_answer)\n",
    "        predicted_answers.append(predicted_answer)\n",
    "\n",
    "    # Now you have both true answers and predicted answers for the selected subset\n",
    "    return true_answers, predicted_answers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "true_answers, predicted_answers = evaluate_model_on_subset(test_dataset, percentage_set=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 Score\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_f1(true_answers, predicted_answers):\n",
    "    # Tokenize the answers by splitting into words\n",
    "    true_answers = [answer.split() for answer in true_answers]\n",
    "    predicted_answers = [answer.split() for answer in predicted_answers]\n",
    "    \n",
    "    # Compute F1 for each pair of true and predicted answers\n",
    "    f1_scores = []\n",
    "    for true, pred in zip(true_answers, predicted_answers):\n",
    "        # Compute precision and recall\n",
    "        intersection = len(set(true) & set(pred))\n",
    "        if len(true) + len(pred) == 0:\n",
    "            f1_scores.append(0.0)\n",
    "        else:\n",
    "            precision = intersection / len(pred) if len(pred) > 0 else 0\n",
    "            recall = intersection / len(true) if len(true) > 0 else 0\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "    # Return average F1 score\n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "# Example usage\n",
    "f1 = compute_f1(true_answers, predicted_answers)\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BlEU Score\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def compute_bleu(true_answers, predicted_answers):\n",
    "    bleu_scores = []\n",
    "    for true, pred in zip(true_answers, predicted_answers):\n",
    "        true_tokens = true.split()  # Tokenize true answer into words\n",
    "        pred_tokens = pred.split()  # Tokenize predicted answer into words\n",
    "        bleu_score = sentence_bleu([true_tokens], pred_tokens)  # Compute BLEU score\n",
    "        bleu_scores.append(bleu_score)\n",
    "    \n",
    "    # Return average BLEU score\n",
    "    return np.mean(bleu_scores)\n",
    "\n",
    "# Example usage\n",
    "bleu = compute_bleu(true_answers, predicted_answers)\n",
    "print(f\"BLEU Score: {bleu}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rouge_score\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def compute_rouge(true_answers, predicted_answers):\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "    rouge_scores = []\n",
    "    for true, pred in zip(true_answers, predicted_answers):\n",
    "        scores = scorer.score(true, pred)  # Get the ROUGE scores for each pair\n",
    "        rouge_scores.append(scores)\n",
    "    \n",
    "    # Compute average ROUGE scores (mean of all ROUGE metrics)\n",
    "    rouge1 = np.mean([score[\"rouge1\"].fmeasure for score in rouge_scores])\n",
    "    rouge2 = np.mean([score[\"rouge2\"].fmeasure for score in rouge_scores])\n",
    "    rougeL = np.mean([score[\"rougeL\"].fmeasure for score in rouge_scores])\n",
    "    \n",
    "    return rouge1, rouge2, rougeL\n",
    "\n",
    "# Example usage\n",
    "rouge1, rouge2, rougeL = compute_rouge(true_answers, predicted_answers)\n",
    "print(f\"ROUGE-1 Score: {rouge1}\")\n",
    "print(f\"ROUGE-2 Score: {rouge2}\")\n",
    "print(f\"ROUGE-L Score: {rougeL}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1T5-zKWM_5OD21QHwXHiV9ixTRR7k3iB9",
     "timestamp": 1732877852976
    },
    {
     "file_id": "15OyFkGoCImV9dSsewU1wa2JuKB4-mDE_",
     "timestamp": 1727333957154
    },
    {
     "file_id": "1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp",
     "timestamp": 1724446900756
    },
    {
     "file_id": "135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp",
     "timestamp": 1721714808667
    },
    {
     "file_id": "10NbwlsRChbma1v55m8LAPYG15uQv6HLo",
     "timestamp": 1713459337061
    },
    {
     "file_id": "1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_",
     "timestamp": 1708958229810
    },
    {
     "file_id": "1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5",
     "timestamp": 1703608159823
    },
    {
     "file_id": "1oW55fBmwzCOrBVX66RcpptL3a99qWBxb",
     "timestamp": 1702886138876
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

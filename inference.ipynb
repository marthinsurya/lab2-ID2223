{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"2eSvM9zX_2d3","executionInfo":{"status":"ok","timestamp":1733481542321,"user_tz":-60,"elapsed":29893,"user":{"displayName":"Marthin Surya Setiawan","userId":"04398797140247513367"}}},"outputs":[],"source":["%%capture\n","!pip install unsloth\n","# Also get the latest nightly Unsloth!\n","#!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n","\n","\n","# Unsloth achieves up to 2x faster fine-tuning speeds compared to traditional methods, with a significant reduction in memory usage (up to 70%).\n","# This makes it suitable for environments with constrained computational resources, like Google Colab or low-end GPUs​\n","\n","# Unsloth leverages LoRA (Low-Rank Adaptation), which modifies only a small fraction (1-10%) of a model's parameters during training,\n","# instead of fine-tuning the entire model. This drastically reduces the computational and memory requirements while achieving comparable performance.\n","# It allows models to adapt to domain-specific tasks without retraining the entire network, enabling faster iterations and greater flexibility.\n","\n","# By supporting 4-bit quantization, Unsloth minimizes memory usage during training and inference.\n","# Quantization reduces the precision of the weights and activations, which reduces memory demands and accelerates computation while preserving accuracy​.\n","#     - the weights of the models use only 4-bits representation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"19hXhrohcbuR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733471029863,"user_tz":-60,"elapsed":31242,"user":{"displayName":"Marthin Surya Setiawan","userId":"04398797140247513367"}},"outputId":"4cc58954-d676-4d6f-db7e-fd4d00599334"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","'Copy of Llama-3.2 1B+3B Conversational + 2x faster finetuning.ipynb'\n"," inference.ipynb\n"," log\n"," lora_model1\n"," lora_model2\n"," outputs\n"," outputs-wo-eval\n","'Yet another copy of Llama-3.2 1B+3B Conversational + 2x faster finetuning.ipynb'\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","#model_path = \"/content/drive/My Drive/Colab Notebooks/lora_model2\"\n","\n","!ls \"/content/drive/My Drive/Colab Notebooks\""]},{"cell_type":"code","source":["model_path = \"emeses/lab2_model\""],"metadata":{"id":"bQfnXZSMf_I8","executionInfo":{"status":"ok","timestamp":1733481542322,"user_tz":-60,"elapsed":9,"user":{"displayName":"Marthin Surya Setiawan","userId":"04398797140247513367"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22580,"status":"ok","timestamp":1733481684469,"user":{"displayName":"Marthin Surya Setiawan","userId":"04398797140247513367"},"user_tz":-60},"id":"QmUBVEnvCDJv","outputId":"7bf3d032-97dc-4a6c-ce57-7bb6549e15f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth 2024.12.2: Fast Llama patching. Transformers:4.46.3.\n","   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","One option could be to make a beef lettuce wrap. Simply cut the lettuce in half lengthwise and use it as the wrap, place slices of beef and any additional desired toppings inside.\n","\n","Alternatively, you could make a chicken salad by slicing the chicken and mixing it with lettuce, as well as any other desired ingredients, such as mayonnaise or spices.\n","\n","Please let me know if you have any specific preferences or restrictions to help you make a better decision.<|eot_id|>\n"]}],"source":["from unsloth import FastLanguageModel\n","import torch\n","\n","\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = model_path, # YOUR MODEL YOU USED FOR TRAINING\n","    max_seq_length = 2048,\n","    #max_seq_length = 512,\n","    dtype = None,\n","    #dtype = torch.bfloat32,\n","    load_in_4bit = True,\n","    #load_in_4bit = False,\n",")\n","#).to(\"cpu\")\n","\n","FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","\n","messages = [\n","    {\"role\": \"user\", \"content\": \"I have beef, chicken, and lettuce, what can I make?\"},\n","]\n","inputs = tokenizer.apply_chat_template(\n","    messages,\n","    tokenize = True,\n","    add_generation_prompt = True, # Must add for generation\n","    return_tensors = \"pt\",\n",").to(\"cuda\")\n","#).to(\"cpu\")\n","\n","from transformers import TextStreamer\n","text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n","_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n","                   use_cache = True, temperature = 1.5, min_p = 0.1)"]},{"cell_type":"markdown","metadata":{"id":"K9CBpiISFa6C"},"source":["We now use `standardize_sharegpt` to convert ShareGPT style datasets into HuggingFace's generic format. This changes the dataset from looking like:\n","```\n","{\"from\": \"system\", \"value\": \"You are an assistant\"}\n","{\"from\": \"human\", \"value\": \"What is 2+2?\"}\n","{\"from\": \"gpt\", \"value\": \"It's 4.\"}\n","```\n","to\n","```\n","{\"role\": \"system\", \"content\": \"You are an assistant\"}\n","{\"role\": \"user\", \"content\": \"What is 2+2?\"}\n","{\"role\": \"assistant\", \"content\": \"It's 4.\"}\n","```"]},{"cell_type":"code","source":["#print(\"Loaded model path:\", model_path)\n","print(\"Model configuration:\", model.config)\n","print(\"Tokenizer vocab size:\", tokenizer.vocab_size)\n","print(\"Tokenizer config:\", tokenizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OSSD1_gB-TEq","executionInfo":{"status":"ok","timestamp":1733471876727,"user_tz":-60,"elapsed":384,"user":{"displayName":"Marthin Surya Setiawan","userId":"04398797140247513367"}},"outputId":"e5097d6d-731e-41d5-f07f-8889acd50882"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model configuration: LlamaConfig {\n","  \"_attn_implementation_autoset\": true,\n","  \"_name_or_path\": \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"head_dim\": 128,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 8192,\n","  \"max_position_embeddings\": 131072,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 24,\n","  \"num_hidden_layers\": 28,\n","  \"num_key_value_heads\": 8,\n","  \"pad_token_id\": 128004,\n","  \"pretraining_tp\": 1,\n","  \"quantization_config\": {\n","    \"bnb_4bit_compute_dtype\": \"float16\",\n","    \"bnb_4bit_quant_type\": \"nf4\",\n","    \"bnb_4bit_use_double_quant\": true,\n","    \"llm_int8_enable_fp32_cpu_offload\": false,\n","    \"llm_int8_has_fp16_weight\": false,\n","    \"llm_int8_skip_modules\": null,\n","    \"llm_int8_threshold\": 6.0,\n","    \"load_in_4bit\": true,\n","    \"load_in_8bit\": false,\n","    \"quant_method\": \"bitsandbytes\"\n","  },\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": {\n","    \"factor\": 32.0,\n","    \"high_freq_factor\": 4.0,\n","    \"low_freq_factor\": 1.0,\n","    \"original_max_position_embeddings\": 8192,\n","    \"rope_type\": \"llama3\"\n","  },\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": true,\n","  \"torch_dtype\": \"float16\",\n","  \"transformers_version\": \"4.46.3\",\n","  \"unsloth_version\": \"2024.12.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}\n","\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5262,"status":"ok","timestamp":1733472007666,"user":{"displayName":"Marthin Surya Setiawan","userId":"04398797140247513367"},"user_tz":-60},"id":"MKX_XKs_BNZR","outputId":"a167186f-857b-4c12-960c-a94aa969998a"},"outputs":[{"output_type":"stream","name":"stdout","text":["No, I'm not a singer, but I can generate songs for you if you'd like. I have a vast music library to draw from and can help with a specific request. Just let me know the type of song or the mood you'd like to convey.<|eot_id|>\n"]}],"source":["\n","    from unsloth import FastLanguageModel\n","\n","    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","\n","messages = [\n","    {\"role\": \"user\", \"content\": \"Are you a singer?\"},\n","]\n","inputs = tokenizer.apply_chat_template(\n","    messages,\n","    tokenize = True,\n","    add_generation_prompt = True, # Must add for generation\n","    return_tensors = \"pt\",\n",").to(\"cuda\")\n","\n","from transformers import TextStreamer\n","text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n","_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n","                   use_cache = True, temperature = 1.5, min_p = 0.1)"]},{"cell_type":"markdown","metadata":{"id":"QQMjaNrjsU5_"},"source":["You can also use Hugging Face's `AutoModelForPeftCausalLM`. Only use this if you do not have `unsloth` installed. It can be hopelessly slow, since `4bit` model downloading is not supported, and Unsloth's **inference is 2x faster**."]},{"cell_type":"markdown","metadata":{"id":"bDp0zNpwe6U_"},"source":["Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in `llama.cpp` or a UI based system like `GPT4All`. You can install GPT4All by going [here](https://gpt4all.io/index.html).\n","\n","**[NEW] Try 2x faster inference in a free Colab for Llama-3.1 8b Instruct [here](https://colab.research.google.com/drive/1T-YBVfnphoVc8E2E854qF3jdia2Ll2W2?usp=sharing)**"]},{"cell_type":"markdown","metadata":{"id":"Zt9CHJqO6p30"},"source":["And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/u54VK8m8tk) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n","\n","Some other links:\n","1. Zephyr DPO 2x faster [free Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)\n","2. Llama 7b 2x faster [free Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)\n","3. TinyLlama 4x faster full Alpaca 52K in 1 hour [free Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)\n","4. CodeLlama 34b 2x faster [A100 on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)\n","5. Mistral 7b [free Kaggle version](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)\n","6. We also did a [blog](https://huggingface.co/blog/unsloth-trl) with 🤗 HuggingFace, and we're in the TRL [docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth)!\n","7. `ChatML` for ShareGPT datasets, [conversational notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing)\n","8. Text completions like novel writing [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)\n","9. [**NEW**] We make Phi-3 Medium / Mini **2x faster**! See our [Phi-3 Medium notebook](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing)\n","10. [**NEW**] We make Gemma-2 9b / 27b **2x faster**! See our [Gemma-2 9b notebook](https://colab.research.google.com/drive/1vIrqH5uYDQwsJ4-OO3DErvuv4pBgVwk4?usp=sharing)\n","11. [**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)\n","12. [**NEW**] We make Mistral NeMo 12B 2x faster and fit in under 12GB of VRAM! [Mistral NeMo notebook](https://colab.research.google.com/drive/17d3U-CAIwzmbDRqbZ9NnpHxCkmXB6LZ0?usp=sharing)\n","\n","<div class=\"align-center\">\n","  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n","  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n","  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Support our work if you can! Thanks!\n","</div>"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1T5-zKWM_5OD21QHwXHiV9ixTRR7k3iB9","timestamp":1732877852976},{"file_id":"15OyFkGoCImV9dSsewU1wa2JuKB4-mDE_","timestamp":1727333957154},{"file_id":"1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp","timestamp":1724446900756},{"file_id":"135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp","timestamp":1721714808667},{"file_id":"10NbwlsRChbma1v55m8LAPYG15uQv6HLo","timestamp":1713459337061},{"file_id":"1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_","timestamp":1708958229810},{"file_id":"1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5","timestamp":1703608159823},{"file_id":"1oW55fBmwzCOrBVX66RcpptL3a99qWBxb","timestamp":1702886138876}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":0}